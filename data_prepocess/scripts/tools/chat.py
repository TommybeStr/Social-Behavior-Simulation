from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda"  # the device to load the model onto

tokenizer = AutoTokenizer.from_pretrained("/home/zss/Social_Behavior_Simulation/checkpoints/default/global_step_2079")
model = AutoModelForCausalLM.from_pretrained("/home/zss/Social_Behavior_Simulation/checkpoints/default/global_step_2079", device_map="auto").eval()

# åˆå§‹åŒ–å¤šè½®å¯¹è¯çš„æ¶ˆæ¯åˆ—è¡¨
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç¤¾äº¤åª’ä½“äº’åŠ¨é¢„æµ‹ä¸“å®¶ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥åšæ–‡çš„å…·ä½“å†…å®¹ï¼Œé¢„æµ‹è¯¥æ¡åšæ–‡çš„äº’åŠ¨æƒ…å†µã€‚ä½ ç°åœ¨æ”¶åˆ°çš„è¾“å…¥åŒ…æ‹¬ä»¥ä¸‹å­—æ®µï¼š- user_name: åŸå§‹å‘å¸ƒè€…ç”¨æˆ·å - user_interests: åŸå§‹å‘å¸ƒè€…å…´è¶£ - content: åšæ–‡æ­£æ–‡ - depth: åšæ–‡åœ¨ç½‘ç»œä¸­çš„æ·±åº¦ - historical_interactors: å†å²æ´»è·ƒç”¨æˆ· - potential_interactors: æ½œåœ¨æ´»è·ƒç”¨æˆ·åˆ—è¡¨ï¼ˆä½ åªèƒ½ä»ä¸­é€‰äººè¿›è¡Œé¢„æµ‹ï¼‰ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸å…è®¸åŒ…å«ä»»ä½•è§£é‡Šæ€§å†…å®¹ï¼Œä¹Ÿä¸è¦å±•ç¤ºæ¨ç†è¿‡ç¨‹ï¼š[{\\\"user_name\\\": \\\"ç”¨æˆ·åï¼ˆæ¥è‡ªpotential_interactorsï¼‰\\\", \\\"content\\\": \\\"é¢„æµ‹çš„è¯„è®ºå†…å®¹\\\", \\\"type\\\": \\\"è¯„è®º æˆ– è½¬å‘\\\"}, ...] æ³¨æ„äº‹é¡¹ï¼š1. ä½ å¿…é¡»ä¸”åªèƒ½ä» potential_interactors ä¸­é€‰æ‹©ç”¨æˆ·å¡«å…¥è¾“å‡ºç»“æœï¼›2. type å­—æ®µåªèƒ½ä¸º \\\"è¯„è®º\\\" æˆ– \\\"è½¬å‘\\\"ï¼›3. ä¸å…è®¸æ·»åŠ ä»»ä½•è¯´æ˜ã€ç†ç”±ã€åˆ†æç­‰å†…å®¹ï¼›4. è¾“å‡ºå¿…é¡»æ˜¯ä¸”åªå«ä¸€ä¸ªåˆæ³•çš„JSONæ•°ç»„ç»“æ„ã€‚\'"}
    #{"role": "system", "content": "ä½ æ˜¯é€šä¹‰åƒé—®ï¼Œä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹"}
]

print("æ¬¢è¿ä½¿ç”¨Qwenå¤šè½®å¯¹è¯åŠ©æ‰‹ã€‚è¾“å…¥ 'exit' ç»“æŸå¯¹è¯ã€‚")
while True:
    # è·å–ç”¨æˆ·è¾“å…¥
    user_input = input("User: ")
    if user_input.lower() in ["exit", "quit"]:
        print("å¯¹è¯ç»“æŸã€‚")
        break

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
    messages.append({"role": "user", "content": user_input})

    # å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œæ·»åŠ assistantçš„ç”Ÿæˆæç¤º
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    # æ¨¡å‹ç”Ÿæˆå›å¤
    generated_ids = model.generate(
    input_ids=model_inputs.input_ids,
    attention_mask=model_inputs.attention_mask,  # ğŸ”§ æ˜¾å¼ä¼ å…¥ attention mask
    max_new_tokens=2048
)
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    # æ‰“å°æ¨¡å‹å›å¤
    print(f"Qwen: {response}")

    # æ·»åŠ æ¨¡å‹å›å¤åˆ°å¯¹è¯å†å²
    messages.append({"role": "assistant", "content": response})
