from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda"  # the device to load the model onto

tokenizer = AutoTokenizer.from_pretrained("/home/zss/Social_Behavior_Simulation/checkpoints/default/renew_11.21/tokenizer_with_spans")
model = AutoModelForCausalLM.from_pretrained("/home/zss/Social_Behavior_Simulation/checkpoints/default/renew_11.21/global_step_3200", device_map="auto").eval()


PSEP_TOKEN = "<|psep|>"

# <POTENTIAL_SPANS> æˆå¯¹åŒ…è£¹ï¼ˆæ˜ç¡®æ”¾åœ¨ user.content æœ«å°¾ï¼‰
PSEP_BLOCK_START = "\n<POTENTIAL_SPANS>\n"
PSEP_BLOCK_END   = "\n</POTENTIAL_SPANS>\n"

# è¯„è®ºå†…å®¹å®šä½å“¨å…µï¼ˆä¾›è®­ç»ƒå™¨ç²¾ç¡®å¯¹é½åˆ°â€œè¯„è®ºçš„ content æ–‡æœ¬â€ï¼‰
CSTART_TOKEN = "<|cstart|>"
CEND_TOKEN   = "<|cend|>"
SYSTEM_PROMPT = f'''ä½ æ˜¯ç¤¾äº¤åª’ä½“äº’åŠ¨é¢„æµ‹ä¸“å®¶ã€‚è¯·ä¸¥æ ¼ä¾æ® user æ¶ˆæ¯ä¸­çš„æ ‡æ³¨å­—æ®µè¿›è¡Œåˆ¤æ–­ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªè¦†ç›–å…¨éƒ¨å€™é€‰çš„ JSON æ•°ç»„ï¼ˆé¡ºåºå¿…é¡»ä¸¥æ ¼ä¸å€™é€‰é¡ºåºä¸€è‡´ï¼‰ã€‚

ã€ã€è¾“å…¥å­—æ®µï¼ˆå•æ ·æœ¬ JSONï¼‰ã€‘
- usernameï¼šä½œè€…
- interestsï¼šä½œè€…å…´è¶£ï¼ˆæ•°ç»„ï¼‰
- contentï¼šæ­£æ–‡æ–‡æœ¬ã€‚
- historicalinteractorsï¼šä¸ä½œè€…å†å²ä¸Šå‘ç”Ÿè¿‡äº’åŠ¨çš„ç”¨æˆ·ååˆ—è¡¨ã€‚æ³¨æ„ï¼šå…¶æœ«å°¾ä¼šè¿½åŠ ä¸€ä¸ªç‰¹æ®Šæ®µè½ `<POTENTIAL_SPANS>`ï¼Œç”¨äºæä¾›å€™é€‰äººä¿¡æ¯ã€‚

ã€å…³äº <POTENTIAL_SPANS>ã€‘
- `<POTENTIAL_SPANS>` ç´§è·Ÿåœ¨historicalinteractorsæœ«å°¾ï¼Œå¹¶ä»¥ `</POTENTIAL_SPANS>` ç»“æŸï¼ˆä¸¥æ ¼æˆå¯¹ï¼‰ã€‚
- å…¶ä¸­æ¯ä¸ªå€™é€‰ç”¨æˆå¯¹åˆ†éš”ç¬¦ `{PSEP_TOKEN}` åŒ…è£¹ï¼š`{PSEP_TOKEN}{{å€™é€‰JSON}}{PSEP_TOKEN}`ã€‚
- å€™é€‰ JSON ä¸¥æ ¼åŒ…å«ï¼š{{"user_name": å€™é€‰äºº, "interests": å€™é€‰äººå…´è¶£, "depth": å±‚çº§}}ã€‚
- è¿™äº›å€™é€‰å—çš„å…ˆåé¡ºåºå³ä¸ºè¯„åˆ†ç±»ä¸è¾“å‡ºé¡ºåºçš„å”¯ä¸€ä¾æ®ï¼›ç¦æ­¢é‡æ’ã€ä¸¢å¤±æˆ–å¢æ·»ã€‚

ã€å”¯ä¸€è¾“å‡ºï¼ˆä¸¥æ ¼æ ¼å¼ï¼‰ã€‘
- è¾“å‡ºä¸€ä¸ª JSON æ•°ç»„ï¼Œé•¿åº¦ç­‰äºå€™é€‰æ•°é‡ï¼Œé¡ºåºä¸ <POTENTIAL_SPANS> ä¸­å€™é€‰é¡ºåºä¸€è‡´ã€‚
- æ•°ç»„å…ƒç´ ç»“æ„ï¼š
  {{"user_name":"...", "content":"{CSTART_TOKEN}...{CEND_TOKEN}", "type":0/1/2}}
  - typeï¼š0=æ— äº’åŠ¨ï¼›1=è¯„è®ºï¼›2=è½¬å‘å¾®åš
  - contentï¼štype=1/2 æ—¶ç”¨ {CSTART_TOKEN}â€¦{CEND_TOKEN} åŒ…è£¹ï¼ˆå¯ä¸ºç©ºä½†æ ‡è®°å¿…é¡»å­˜åœ¨ï¼‰ï¼›type=0 æ—¶è¾“å‡º "{CSTART_TOKEN}{CEND_TOKEN}"ã€‚
- ä»…è¾“å‡ºè¯¥ JSON æ•°ç»„ï¼Œä¸å¾—åŒ…å«è§£é‡Šæˆ–å¤šä½™æ–‡æœ¬ã€‚
- ç¦æ­¢ä½¿ç”¨ {CSTART_TOKEN}/{CEND_TOKEN} ä¹‹å¤–çš„è‡ªé€ æ ‡è®°ã€‚
'''
# åˆå§‹åŒ–å¤šè½®å¯¹è¯çš„æ¶ˆæ¯åˆ—è¡¨
messages = [
    {"role": "system", "content": SYSTEM_PROMPT}

]

print("æ¬¢è¿ä½¿ç”¨Qwenå¤šè½®å¯¹è¯åŠ©æ‰‹ã€‚è¾“å…¥ 'exit' ç»“æŸå¯¹è¯ã€‚")
while True:
    # è·å–ç”¨æˆ·è¾“å…¥
    user_input = input("User: ")
    if user_input.lower() in ["exit", "quit"]:
        print("å¯¹è¯ç»“æŸã€‚")
        break

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
    messages.append({"role": "user", "content": user_input})

    # å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œæ·»åŠ assistantçš„ç”Ÿæˆæç¤º
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    # æ¨¡å‹ç”Ÿæˆå›å¤
    generated_ids = model.generate(
    input_ids=model_inputs.input_ids,
    attention_mask=model_inputs.attention_mask,  # ğŸ”§ æ˜¾å¼ä¼ å…¥ attention mask
    max_new_tokens=4096
)
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    # æ‰“å°æ¨¡å‹å›å¤
    print(f"Qwen: {response}")

    # æ·»åŠ æ¨¡å‹å›å¤åˆ°å¯¹è¯å†å²
    messages.append({"role": "assistant", "content": response})
