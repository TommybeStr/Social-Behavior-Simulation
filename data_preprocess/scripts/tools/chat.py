from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda"  # the device to load the model onto

tokenizer = AutoTokenizer.from_pretrained("/home/zss/Social_Behavior_Simulation/Qwen2.5-3B-Instruct")
model = AutoModelForCausalLM.from_pretrained("/home/zss/Social_Behavior_Simulation/Qwen2.5-3B-Instruct", device_map="auto").eval()


PSEP_TOKEN = "<|psep|>"

# <POTENTIAL_SPANS> æˆå¯¹åŒ…è£¹ï¼ˆæ˜ç¡®æ”¾åœ¨ user.content æœ«å°¾ï¼‰
PSEP_BLOCK_START = "\n<POTENTIAL_SPANS>\n"
PSEP_BLOCK_END   = "\n</POTENTIAL_SPANS>\n"

# è¯„è®ºå†…å®¹å®šä½å“¨å…µï¼ˆä¾›è®­ç»ƒå™¨ç²¾ç¡®å¯¹é½åˆ°â€œè¯„è®ºçš„ content æ–‡æœ¬â€ï¼‰
CSTART_TOKEN = "<|cstart|>"
CEND_TOKEN   = "<|cend|>"
SYSTEM_PROMPT = f'''ä½ æ˜¯ç¤¾äº¤åª’ä½“äº’åŠ¨é¢„æµ‹ä¸“å®¶ã€‚è¯·åŸºäºç»™å®šçš„å•æ¡æ ·æœ¬ï¼ˆä¸€ä¸ª JSON å¯¹è±¡ï¼‰å¯¹ã€Œæ¯ä¸ªå€™é€‰äº’åŠ¨å¯¹è±¡ã€åˆ†åˆ«è¿›è¡Œä¸‰åˆ†ç±»é¢„æµ‹ï¼š0=æ— äº’åŠ¨ï¼Œ1=è¯„è®ºï¼Œ2=è½¬å‘å¾®åšï¼›è‹¥ä¸ºè¯„è®ºæˆ–è½¬å‘ï¼Œå¯ç»™å‡ºç®€çŸ­å†…å®¹ã€‚æœ€ç»ˆä»…è¾“å‡ºä¸€ä¸ªè¦†ç›–å…¨éƒ¨å€™é€‰çš„ JSON æ•°ç»„ï¼Œé¡ºåºéœ€ä¸å€™é€‰é¡ºåºä¸¥æ ¼ä¸€è‡´ã€‚

ã€è¾“å…¥å­—æ®µï¼ˆå•æ ·æœ¬ JSONï¼‰ã€‘
- user_nameï¼šä½œè€…
- interestsï¼šä½œè€…å…´è¶£ï¼ˆæ•°ç»„ï¼‰
- contentï¼šæ­£æ–‡æ–‡æœ¬ã€‚æ³¨æ„ï¼šæ­£æ–‡æœ«å°¾ä¼šè¿½åŠ ä¸€ä¸ªç‰¹æ®Šæ®µè½ `<POTENTIAL_SPANS>`ï¼Œç”¨äºæä¾›å€™é€‰äººä¿¡æ¯ã€‚
- historical_interactorsï¼šä¸ä½œè€…å†å²ä¸Šå‘ç”Ÿè¿‡äº’åŠ¨çš„ç”¨æˆ·ååˆ—è¡¨

ã€å…³äº <POTENTIAL_SPANS>ã€‘
- `<POTENTIAL_SPANS>` ç´§è·Ÿåœ¨ content æœ«å°¾ï¼Œå¹¶ä»¥ `</POTENTIAL_SPANS>` ç»“æŸï¼ˆä¸¥æ ¼æˆå¯¹ï¼‰ã€‚
- å…¶ä¸­æ¯ä¸ªå€™é€‰ç”¨æˆå¯¹åˆ†éš”ç¬¦ `{PSEP_TOKEN}` åŒ…è£¹ï¼š`{PSEP_TOKEN}{{å€™é€‰JSON}}{PSEP_TOKEN}`ã€‚
- å€™é€‰ JSON ä¸¥æ ¼åŒ…å«ï¼š`{{"user_name": "...", "interests": [...], "depth": æ•´æ•°}}`ã€‚
- è¿™äº›å€™é€‰å—çš„å…ˆåé¡ºåºå³ä¸ºè¯„åˆ†ç±»ä¸è¾“å‡ºé¡ºåºçš„å”¯ä¸€ä¾æ®ï¼›ç¦æ­¢é‡æ’ã€ä¸¢å¤±æˆ–å¢æ·»ã€‚

ã€å”¯ä¸€è¾“å‡ºï¼ˆä¸¥æ ¼æ ¼å¼ï¼‰ã€‘
- è¾“å‡ºä¸€ä¸ª JSON æ•°ç»„ï¼Œé•¿åº¦ç­‰äºå€™é€‰æ•°é‡ï¼Œé¡ºåºä¸ `<POTENTIAL_SPANS>` ä¸­å€™é€‰å—é¡ºåºä¸€è‡´ã€‚
- æ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ çš„ç»“æ„ä¸ºï¼š
  `{{"user_name": "...", "content": "{CSTART_TOKEN}...{CEND_TOKEN}", "type": 0/1/2}}`
  - typeï¼š0=æ— äº’åŠ¨ï¼›1=è¯„è®ºï¼›2=è½¬å‘å¾®åš
  - contentï¼š
      * å½“ type=1/2 æ—¶ï¼Œç”¨ `{CSTART_TOKEN}è¯„è®ºæˆ–è½¬å‘è¯­{CEND_TOKEN}` åŒ…è£¹ï¼ˆå…è®¸ä¸ºç©ºå†…å®¹ä½†å¿…é¡»ä¿ç•™è¿™å¯¹æ ‡è®°ï¼‰ï¼›
      * å½“ type=0 æ—¶ï¼Œå¿…é¡»è¾“å‡º `"{CSTART_TOKEN}{CEND_TOKEN}"`ï¼ˆç©ºå†…å®¹çš„æˆå¯¹æ ‡è®°ï¼‰ã€‚
- ä¸¥ç¦è¾“å‡º `{CSTART_TOKEN}` æˆ– `{CEND_TOKEN}` ä¹‹å¤–çš„è‡ªé€ æ ‡è®°ã€‚

ã€åˆ¤å®šè¦ç‚¹ï¼ˆé€å€™é€‰ç‹¬ç«‹æ‰“åˆ†ï¼‰ã€‘
1) å…´è¶£åŒ¹é…ï¼šä½œè€… interests ä¸å€™é€‰ interests çš„é‡åˆæˆ–ä¸»é¢˜ç›¸å…³åº¦ã€‚
2) å†å²å…³ç³»ï¼šhistorical_interactors æ˜¯å¦åŒ…å«è¯¥å€™é€‰åŠå…¶å¼º/å¼±å…³ç³»è¿¹è±¡ã€‚
3) è¯­ä¹‰å…³è”ï¼šæ­£æ–‡ content ä¸å€™é€‰å…´è¶£æˆ–å†å²è¯é¢˜çš„è¯­ä¹‰ç›¸å…³åº¦ã€‚
4) å±‚çº§ depth è¡°å‡ï¼š`depth` è¶Šæ·±ï¼Œäº’åŠ¨æ¦‚ç‡é€šå¸¸è¶Šä½ï¼Œåº”ä½“ç°è¡°å‡è¶‹åŠ¿ã€‚
5) ä¸è‡†æµ‹å¤–éƒ¨äº‹å®ï¼šä»…ä¾æ®æ ·æœ¬æä¾›çš„ä¿¡æ¯è¿›è¡Œåˆ¤æ–­ã€‚

ã€ç¡¬æ€§è§„åˆ™ã€‘
- åªèƒ½ä½¿ç”¨ `<POTENTIAL_SPANS>` ä¸­å‡ºç°çš„ `user_name`ï¼Œä¸¥ç¦è™šæ„æˆ–é—æ¼ã€‚
- é¡ºåºå¿…é¡»ä¸ `<POTENTIAL_SPANS>` ä¸­å€™é€‰é¡ºåºå®Œå…¨ä¸€è‡´ã€‚
- ä»…è¾“å‡ºä¸Šè¿° JSON æ•°ç»„ï¼›ä¸å¾—è¾“å‡ºä»»ä½•è§£é‡Šã€å‰åç¼€æˆ–é¢å¤–æ–‡æœ¬ã€‚
- JSON å¿…é¡»å¯è§£æï¼ˆåŒå¼•å·ã€é€—å·ä¸æ‹¬å·åˆæ³•ï¼‰ã€‚
'''
# åˆå§‹åŒ–å¤šè½®å¯¹è¯çš„æ¶ˆæ¯åˆ—è¡¨
messages = [
    {"role": "system", "content": SYSTEM_PROMPT}

]

print("æ¬¢è¿ä½¿ç”¨Qwenå¤šè½®å¯¹è¯åŠ©æ‰‹ã€‚è¾“å…¥ 'exit' ç»“æŸå¯¹è¯ã€‚")
while True:
    # è·å–ç”¨æˆ·è¾“å…¥
    user_input = input("User: ")
    if user_input.lower() in ["exit", "quit"]:
        print("å¯¹è¯ç»“æŸã€‚")
        break

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
    messages.append({"role": "user", "content": user_input})

    # å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œæ·»åŠ assistantçš„ç”Ÿæˆæç¤º
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    # æ¨¡å‹ç”Ÿæˆå›å¤
    generated_ids = model.generate(
    input_ids=model_inputs.input_ids,
    attention_mask=model_inputs.attention_mask,  # ğŸ”§ æ˜¾å¼ä¼ å…¥ attention mask
    max_new_tokens=4096
)
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    # æ‰“å°æ¨¡å‹å›å¤
    print(f"Qwen: {response}")

    # æ·»åŠ æ¨¡å‹å›å¤åˆ°å¯¹è¯å†å²
    messages.append({"role": "assistant", "content": response})
